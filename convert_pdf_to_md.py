import os
import sys
from pathlib import Path
<<<<<<< HEAD
from pdf_craft import create_pdf_page_extractor, MarkDownWriter, ExtractedTableFormat
=======
from pdf_craft import create_pdf_page_extractor, MarkDownWriter, ExtractedTableFormat, analyse, CorrectionMode
>>>>>>> c5fd878ef717c3e7aee6fd715ea2cfcec3472816
import time
import re
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import threading
import json
import pickle
import psutil
import gc
import traceback
import logging

<<<<<<< HEAD
# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('conversion.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def detect_gpu():
    """æª¢æ¸¬å¯ç”¨çš„GPUè¨­å‚™"""
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            print(f"ğŸš€ æª¢æ¸¬åˆ° {gpu_count} å€‹GPUè¨­å‚™")
            print(f"   GPU 0: {gpu_name}")
            print(f"   GPUè¨˜æ†¶é«”: {gpu_memory:.1f} GB")
            print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
            return "cuda", gpu_count, gpu_memory
        else:
            print("âš ï¸  æœªæª¢æ¸¬åˆ°å¯ç”¨çš„GPUï¼Œå°‡ä½¿ç”¨CPU")
            return "cpu", 0, 0
    except ImportError:
        print("âš ï¸  PyTorchæœªå®‰è£ï¼Œç„¡æ³•æª¢æ¸¬GPUï¼Œå°‡ä½¿ç”¨CPU")
        return "cpu", 0, 0
    except Exception as e:
        print(f"âš ï¸  GPUæª¢æ¸¬å¤±æ•—: {e}ï¼Œå°‡ä½¿ç”¨CPU")
        return "cpu", 0, 0

def get_optimal_workers(device, gpu_count, gpu_memory):
    """æ ¹æ“šç³»çµ±è³‡æºè¨ˆç®—æœ€ä½³å·¥ä½œé€²ç¨‹æ•¸"""
    cpu_count = mp.cpu_count()
    available_memory = psutil.virtual_memory().available / 1024**3  # GB
    
    print(f"ğŸ’» ç³»çµ±è³‡æºåˆ†æ:")
    print(f"   CPUæ ¸å¿ƒæ•¸: {cpu_count}")
    print(f"   å¯ç”¨è¨˜æ†¶é«”: {available_memory:.1f} GB")
    
    if device == "cuda":
        # GPUè™•ç†ï¼šå„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨
        if gpu_memory >= 8:  # 8GBä»¥ä¸ŠGPU
            workers = min(6, cpu_count)
        elif gpu_memory >= 6:  # 6-8GB GPU
            workers = min(4, cpu_count)
        elif gpu_memory >= 4:  # 4-6GB GPU (ä½ çš„GTX 1650)
            workers = min(3, cpu_count)  # å¾2å¢åŠ åˆ°3
        else:  # å°æ–¼4GB GPU
            workers = min(2, cpu_count)  # å¾1å¢åŠ åˆ°2
        print(f"   ğŸ¯ GPUæ¨¡å¼ - å»ºè­°å·¥ä½œé€²ç¨‹æ•¸: {workers}")
    else:
        # CPUè™•ç†ï¼šè€ƒæ…®è¨˜æ†¶é«”é™åˆ¶
        if available_memory >= 16:  # 16GBä»¥ä¸Šè¨˜æ†¶é«”
            workers = min(cpu_count, 8)
        elif available_memory >= 8:  # 8-16GBè¨˜æ†¶é«”
            workers = min(cpu_count // 2, 4)
        else:  # å°æ–¼8GBè¨˜æ†¶é«”
            workers = min(cpu_count // 4, 2)
        print(f"   ğŸ¯ CPUæ¨¡å¼ - å»ºè­°å·¥ä½œé€²ç¨‹æ•¸: {workers}")
    
    return max(1, workers)  # è‡³å°‘1å€‹å·¥ä½œé€²ç¨‹

def validate_pdf_file(pdf_path):
    """é©—è­‰PDFæª”æ¡ˆæ˜¯å¦å¯è®€å–"""
    try:
        if not pdf_path.exists():
            return False, "æª”æ¡ˆä¸å­˜åœ¨"
        
        if pdf_path.stat().st_size == 0:
            return False, "æª”æ¡ˆå¤§å°ç‚º0"
        
        # æª¢æŸ¥æª”æ¡ˆé ­éƒ¨æ˜¯å¦ç‚ºPDFæ ¼å¼
        with open(pdf_path, 'rb') as f:
            header = f.read(4)
            if header != b'%PDF':
                return False, "ä¸æ˜¯æœ‰æ•ˆçš„PDFæª”æ¡ˆ"
        
        return True, "æª”æ¡ˆé©—è­‰é€šé"
    except Exception as e:
        return False, f"æª”æ¡ˆé©—è­‰å¤±æ•—: {str(e)}"

=======
>>>>>>> c5fd878ef717c3e7aee6fd715ea2cfcec3472816
def process_math_formulas(text):
    """è™•ç†æ•¸å­¸å…¬å¼ï¼Œè½‰æ›ç‚ºLaTeXæ ¼å¼"""
    math_patterns = [
        (r'(\d+)/(\d+)', r'\\frac{\1}{\2}'),
        (r'âˆš(\w+)', r'\\sqrt{\1}'),
        (r'(\w+)\^(\w+)', r'\1^{\2}'),
        (r'Î±', r'\\alpha'),
        (r'Î²', r'\\beta'),
        (r'Î³', r'\\gamma'),
        (r'Ï€', r'\\pi'),
        (r'Î¸', r'\\theta'),
        (r'Â±', r'\\pm'),
        (r'â‰¤', r'\\leq'),
        (r'â‰¥', r'\\geq'),
        (r'â‰ ', r'\\neq'),
        (r'âˆ', r'\\infty'),
        (r'\|([^|]+)\|', r'|\1|'),
        (r'ï¼', r'-'),
        (r'â€”', r'-'),
    ]
    processed_text = text
    for pattern, replacement in math_patterns:
        processed_text = re.sub(pattern, replacement, processed_text)
    return processed_text

<<<<<<< HEAD
def convert_single_pdf_worker(args):
    """å–®å€‹PDFè½‰æ›å·¥ä½œå‡½æ•¸ï¼ˆç”¨æ–¼å¤šé€²ç¨‹ï¼‰"""
    pdf_path, output_dir, image_output_dir, device, model_cache_path, encoding, enable_math_processing, enable_multilingual_ocr, extract_table_format, worker_id = args
    
=======
def convert_pdf_to_markdown(pdf_path, output_dir, image_output_dir, extractor, encoding="utf-8", 
                          enable_math_processing=True, enable_multilingual_ocr=True):
    """è½‰æ›å–®å€‹PDFæª”æ¡ˆç‚ºMarkdownï¼Œæ”¯æ´å¤šé‡OCRå’Œæ•¸å­¸å…¬å¼è™•ç†"""
>>>>>>> c5fd878ef717c3e7aee6fd715ea2cfcec3472816
    try:
        # é©—è­‰PDFæª”æ¡ˆ
        is_valid, validation_msg = validate_pdf_file(pdf_path)
        if not is_valid:
            return False, f"å·¥ä½œé€²ç¨‹ {worker_id}: {validation_msg}"
        
        # ç‚ºæ¯å€‹å·¥ä½œé€²ç¨‹å‰µå»ºç¨ç«‹çš„extractor
        try:
            extractor = create_pdf_page_extractor(
                device=device,
                model_dir_path=str(model_cache_path),
                extract_formula=True,
                extract_table_format=extract_table_format,
            )
        except Exception as e:
            return False, f"å·¥ä½œé€²ç¨‹ {worker_id}: PDFè§£æå™¨åˆå§‹åŒ–å¤±æ•— - {str(e)}"
        
        if not extractor:
            return False, f"å·¥ä½œé€²ç¨‹ {worker_id}: PDFè§£æå™¨åˆå§‹åŒ–å¤±æ•—"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        image_output_dir.mkdir(parents=True, exist_ok=True)
        pdf_name = pdf_path.stem
        output_md_path = output_dir / f"{pdf_name}.md"
        
<<<<<<< HEAD
        start_time = time.time()
        
        try:
            with MarkDownWriter(output_md_path, image_output_dir, encoding) as md:
                for block in extractor.extract(str(pdf_path)):
                    if enable_math_processing and hasattr(block, 'text'):
                        block.text = process_math_formulas(block.text)
                    md.write(block)
                    
        except ModuleNotFoundError as module_error:
            if "struct_eqtable" in str(module_error):
                with open(output_md_path, 'w', encoding=encoding) as f:
                    f.write(f"# {pdf_name}\n\n")
                    f.write(f"*æ­¤æª”æ¡ˆå› è¡¨æ ¼è™•ç†æ¨¡çµ„ç¼ºå¤±è€Œç„¡æ³•å®Œæ•´è½‰æ›*\n\n")
                    f.write(f"åŸå§‹æª”æ¡ˆ: {pdf_path.name}\n")
                    f.write(f"è½‰æ›æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                return True, f"å·¥ä½œé€²ç¨‹ {worker_id}: å‰µå»ºåŸºæœ¬æ–‡å­—ç‰ˆæœ¬"
            else:
                return False, f"å·¥ä½œé€²ç¨‹ {worker_id}: æ¨¡çµ„éŒ¯èª¤ - {module_error}"
                
        except Exception as extract_error:
            error_details = traceback.format_exc()
            logging.error(f"PDFæå–å¤±æ•—: {pdf_path.name}\néŒ¯èª¤è©³æƒ…: {error_details}")
            return False, f"å·¥ä½œé€²ç¨‹ {worker_id}: PDFæå–å¤±æ•— - {extract_error}"
            
        elapsed_time = time.time() - start_time
        return True, f"å·¥ä½œé€²ç¨‹ {worker_id}: å®Œæˆè½‰æ› {pdf_name} (è€—æ™‚: {elapsed_time:.2f}ç§’)"
=======
        print(f"ğŸ”„ æ­£åœ¨è½‰æ›: {pdf_path.name}")
        print(f"   ğŸ“ æ•¸å­¸å…¬å¼è™•ç†: {'å•Ÿç”¨' if enable_math_processing else 'åœç”¨'}")
        print(f"   ğŸŒ å¤šèªè¨€OCR: {'å•Ÿç”¨' if enable_multilingual_ocr else 'åœç”¨'}")
        start_time = time.time()
        
        # ç›´æ¥è½‰æ›PDFç‚ºMarkdown
        with MarkDownWriter(output_md_path, image_output_dir, encoding) as md:
            for block in extractor.extract(str(pdf_path)):
                if enable_math_processing and hasattr(block, 'text'):
                    block.text = process_math_formulas(block.text)
                
                # å¯«å…¥å€å¡Š
                md.write(block)
>>>>>>> c5fd878ef717c3e7aee6fd715ea2cfcec3472816
        
    except Exception as e:
        error_details = traceback.format_exc()
        logging.error(f"å·¥ä½œé€²ç¨‹ {worker_id} æœªçŸ¥éŒ¯èª¤: {pdf_path.name}\néŒ¯èª¤è©³æƒ…: {error_details}")
        return False, f"å·¥ä½œé€²ç¨‹ {worker_id}: æœªçŸ¥éŒ¯èª¤ - {str(e)}"
    finally:
        # å„ªåŒ–GPUè¨˜æ†¶é«”æ¸…ç†ç­–ç•¥
        if device == "cuda":
            try:
                import torch
                # åªåœ¨è¨˜æ†¶é«”ä½¿ç”¨è¶…é80%æ™‚æ‰æ¸…ç†
                if torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() > 0.8:
                    torch.cuda.empty_cache()
                    gc.collect()
            except:
                pass

def convert_pdf_to_markdown(pdf_path, output_dir, image_output_dir, extractor, encoding="utf-8", 
                          enable_math_processing=True, enable_multilingual_ocr=True):
    """è½‰æ›å–®å€‹PDFæª”æ¡ˆç‚ºMarkdownï¼Œæ”¯æ´å¤šé‡OCRå’Œæ•¸å­¸å…¬å¼è™•ç†"""
    try:
        # é©—è­‰PDFæª”æ¡ˆ
        is_valid, validation_msg = validate_pdf_file(pdf_path)
        if not is_valid:
            print(f"âŒ æª”æ¡ˆé©—è­‰å¤±æ•—: {pdf_path.name}")
            print(f"   éŒ¯èª¤: {validation_msg}")
            return False, None
        
        output_dir.mkdir(parents=True, exist_ok=True)
        image_output_dir.mkdir(parents=True, exist_ok=True)
        pdf_name = pdf_path.stem
        output_md_path = output_dir / f"{pdf_name}.md"
        print(f"ğŸ”„ æ­£åœ¨è½‰æ›: {pdf_path.name}")
        print(f"   ğŸ“ æ•¸å­¸å…¬å¼è™•ç†: {'å•Ÿç”¨' if enable_math_processing else 'åœç”¨'}")
        print(f"   ğŸŒ å¤šèªè¨€OCR: {'å•Ÿç”¨' if enable_multilingual_ocr else 'åœç”¨'}")
        start_time = time.time()
        try:
            with MarkDownWriter(output_md_path, image_output_dir, encoding) as md:
                for block in extractor.extract(str(pdf_path)):
                    if enable_math_processing and hasattr(block, 'text'):
                        block.text = process_math_formulas(block.text)
                    md.write(block)
        except ModuleNotFoundError as module_error:
            if "struct_eqtable" in str(module_error):
                print(f"   âš ï¸  è·³éæ­¤æª”æ¡ˆ - è¡¨æ ¼è™•ç†æ¨¡çµ„ç¼ºå¤±")
                print(f"   ğŸ“ å‰µå»ºåŸºæœ¬æ–‡å­—ç‰ˆæœ¬...")
                with open(output_md_path, 'w', encoding=encoding) as f:
                    f.write(f"# {pdf_name}\n\n")
                    f.write(f"*æ­¤æª”æ¡ˆå› è¡¨æ ¼è™•ç†æ¨¡çµ„ç¼ºå¤±è€Œç„¡æ³•å®Œæ•´è½‰æ›*\n\n")
                    f.write(f"åŸå§‹æª”æ¡ˆ: {pdf_path.name}\n")
                    f.write(f"è½‰æ›æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                return True, output_md_path
            else:
                raise module_error
        except Exception as extract_error:
            print(f"   âŒ PDFæå–å¤±æ•—: {extract_error}")
            error_details = traceback.format_exc()
            logging.error(f"PDFæå–å¤±æ•—: {pdf_path.name}\néŒ¯èª¤è©³æƒ…: {error_details}")
            raise extract_error
        elapsed_time = time.time() - start_time
        print(f"âœ… å®Œæˆè½‰æ›: {pdf_name} (è€—æ™‚: {elapsed_time:.2f}ç§’)")
        print(f"   è¼¸å‡ºæª”æ¡ˆ: {output_md_path}")
        return True, output_md_path
    except Exception as e:
        print(f"âŒ è½‰æ›å¤±æ•—: {pdf_path.name}")
        print(f"   éŒ¯èª¤è©³æƒ…: {str(e)}")
        print(f"   éŒ¯èª¤é¡å‹: {type(e).__name__}")
        error_details = traceback.format_exc()
        logging.error(f"è½‰æ›å¤±æ•—: {pdf_path.name}\néŒ¯èª¤è©³æƒ…: {error_details}")
        return False, None

<<<<<<< HEAD
def batch_convert_parallel(root_dir, output_base_dir, image_output_dir, model_cache_path, device, encoding, enable_math_processing, enable_multilingual_ocr, extract_table_format, max_workers=None):
    """ä¸¦è¡Œæ‰¹æ¬¡è½‰æ›PDFæª”æ¡ˆ"""
    excluded_subjects = {"Chinese", "English"}
    progress_file = output_base_dir / "conversion_progress.json"
    failed_files_file = output_base_dir / "failed_files.json"
    processed_files = set()
    failed_files = []
=======
def process_subject(subject, base_input_dir, output_base_dir, image_output_dir, model_cache_path, 
                   device, encoding, enable_math_processing, enable_multilingual_ocr, extract_table_format):
    """è™•ç†å–®å€‹ç§‘ç›®çš„æ‰€æœ‰PDFæª”æ¡ˆ"""
    print(f"\n{'='*20} è™•ç† {subject} ç§‘ç›® {'='*20}")
>>>>>>> c5fd878ef717c3e7aee6fd715ea2cfcec3472816
    
    if progress_file.exists():
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
                processed_files = set(progress_data.get('processed_files', []))
                failed_files = progress_data.get('failed_files', [])
                print(f"ğŸ“‹ ç™¼ç¾é€²åº¦æª”æ¡ˆï¼Œå·²è™•ç† {len(processed_files)} å€‹æª”æ¡ˆ")
                print(f"âŒ å¤±æ•—æª”æ¡ˆ: {len(failed_files)} å€‹")
                print(f"ğŸ”„ å°‡å¾æœªè™•ç†çš„æª”æ¡ˆé–‹å§‹ç¹¼çºŒè½‰æ›")
        except Exception as e:
            print(f"âš ï¸  è®€å–é€²åº¦æª”æ¡ˆå¤±æ•—: {e}")
    
    all_pdfs = list(Path(root_dir).rglob("*.pdf"))
    print(f"\nğŸ” æƒæç›®éŒ„: {root_dir}")
    print(f"ğŸ“„ æ‰¾åˆ° {len(all_pdfs)} å€‹PDFæª”æ¡ˆ")
    print(f"ğŸš« æ’é™¤ç§‘ç›®: {', '.join(excluded_subjects)}")
    
<<<<<<< HEAD
    pdfs_to_process = []
    for pdf_path in all_pdfs:
        path_parts = pdf_path.parts
        root_parts = Path(root_dir).parts
        if len(path_parts) > len(root_parts):
            subject_name = path_parts[-2]
            if subject_name not in excluded_subjects:
                if str(pdf_path) not in processed_files:
                    pdfs_to_process.append(pdf_path)
                else:
                    print(f"â­ï¸  è·³éå·²è™•ç†çš„æª”æ¡ˆ: {pdf_path.name}")
            else:
                print(f"â­ï¸  è·³é {subject_name} ç§‘ç›®æª”æ¡ˆ: {pdf_path.name}")
=======
    # åˆå§‹åŒ–PDFè§£æå™¨ï¼ˆæ¯å€‹é€²ç¨‹ç¨ç«‹ï¼‰
    extractor = create_pdf_page_extractor(
        device=device,
        model_dir_path=str(model_cache_path),
        extract_formula=True,
        extract_table_format=extract_table_format,
    )
    
    if not extractor:
        print(f"âŒ {subject} PDFè§£æå™¨åˆå§‹åŒ–å¤±æ•—")
        return subject, 0, len(subject_pdfs)
    
    subject_successful = 0
    subject_failed = 0
    
    for i, pdf_path in enumerate(subject_pdfs, 1):
        print(f"\n[{i}/{len(subject_pdfs)}] è™•ç† {subject} æª”æ¡ˆ...")
        
        success, output_path = convert_pdf_to_markdown(
            pdf_path, 
            subject_output_dir, 
            subject_image_dir, 
            extractor, 
            encoding,
            enable_math_processing=enable_math_processing,
            enable_multilingual_ocr=enable_multilingual_ocr
        )
        
        if success:
            subject_successful += 1
>>>>>>> c5fd878ef717c3e7aee6fd715ea2cfcec3472816
        else:
            print(f"âš ï¸  è·³éè·¯å¾‘çµæ§‹ä¸æ­£ç¢ºçš„æª”æ¡ˆ: {pdf_path}")
    
    # è¨ˆç®—æœ€ä½³å·¥ä½œé€²ç¨‹æ•¸
    device_type, gpu_count, gpu_memory = detect_gpu()
    if max_workers is None:
        max_workers = get_optimal_workers(device_type, gpu_count, gpu_memory)
    
<<<<<<< HEAD
    print(f"âœ… å°‡è™•ç† {len(pdfs_to_process)} å€‹PDFæª”æ¡ˆ")
    if len(pdfs_to_process) == 0:
        print("ğŸ‰ æ‰€æœ‰æª”æ¡ˆéƒ½å·²è™•ç†å®Œæˆï¼")
        return
    
    # å¼·åˆ¶ä½¿ç”¨å–®ä¸€é€²ç¨‹
    max_workers = 1
    print(f"ğŸš€ ä½¿ç”¨ {max_workers} å€‹ä¸¦è¡Œå·¥ä½œé€²ç¨‹")
    
    # æº–å‚™å·¥ä½œä»»å‹™
    tasks = []
    for i, pdf_path in enumerate(pdfs_to_process):
        rel_path = pdf_path.parent.relative_to(root_dir)
        output_dir = output_base_dir / rel_path
        image_dir = image_output_dir / rel_path
        
        task_args = (
            pdf_path,
            output_dir,
            image_dir,
            device,
            model_cache_path,
            encoding,
            enable_math_processing,
            enable_multilingual_ocr,
            extract_table_format,
            i + 1  # worker_id
=======
    return subject, subject_successful, subject_failed

def batch_convert_all_pdfs(root_dir, output_base_dir, image_output_dir, model_cache_path, device, encoding, enable_math_processing, enable_multilingual_ocr, extract_table_format):
    """æ‰¹æ¬¡è½‰æ›æ‰€æœ‰PDFæª”æ¡ˆï¼Œå–®ç·šç¨‹è™•ç†"""
    pdf_files = list(Path(root_dir).rglob("*.pdf"))
    print(f"\nğŸ” å…±æ‰¾åˆ° {len(pdf_files)} å€‹ PDF æª”æ¡ˆæ–¼ {root_dir}")
    
    # åˆå§‹åŒ–PDFè§£æå™¨ï¼ˆå…±ç”¨ï¼‰
    extractor = create_pdf_page_extractor(
        device=device,
        model_dir_path=str(model_cache_path),
        extract_formula=True,
        extract_table_format=extract_table_format,
    )
    
    success_count = 0
    fail_count = 0
    
    for i, pdf_path in enumerate(pdf_files, 1):
        # ä¾æ“š PDF æ‰€åœ¨ç›®éŒ„å»ºç«‹å°æ‡‰è¼¸å‡ºè³‡æ–™å¤¾
        rel_dir = pdf_path.parent.relative_to(root_dir)
        out_dir = output_base_dir / rel_dir
        img_dir = image_output_dir / rel_dir
        print(f"\n[{i}/{len(pdf_files)}] è™•ç† {pdf_path}")
        success, output_path = convert_pdf_to_markdown(
            pdf_path,
            out_dir,
            img_dir,
            extractor,
            encoding=encoding,
            enable_math_processing=enable_math_processing,
            enable_multilingual_ocr=enable_multilingual_ocr
>>>>>>> c5fd878ef717c3e7aee6fd715ea2cfcec3472816
        )
        tasks.append(task_args)
    
    total_success = 0
    total_failed = 0
    output_base_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_task = {executor.submit(convert_single_pdf_worker, task): task for task in tasks}
            
            # è™•ç†å®Œæˆçš„ä»»å‹™
            for i, future in enumerate(as_completed(future_to_task), 1):
                task = future_to_task[future]
                pdf_path = task[0]
                
                try:
                    success, message = future.result()
                    print(f"\n{'='*20} å®Œæˆä»»å‹™ {i}/{len(tasks)} {'='*20}")
                    print(f"ğŸ“„ æª”æ¡ˆ: {pdf_path.name}")
                    print(f"ğŸ“ è·¯å¾‘: {pdf_path}")
                    print(f"ğŸ“‹ çµæœ: {message}")
                    
                    processed_files.add(str(pdf_path))
                    
                    if success:
                        total_success += 1
                        print(f"âœ… æˆåŠŸè½‰æ›: {pdf_path.name}")
                    else:
                        total_failed += 1
                        failed_files.append({
                            'file': str(pdf_path),
                            'error': message,
                            'timestamp': time.time()
                        })
                        print(f"âŒ è½‰æ›å¤±æ•—: {pdf_path.name}")
                    
                    # æ¯è™•ç†10å€‹æª”æ¡ˆä¿å­˜ä¸€æ¬¡é€²åº¦
                    if i % 10 == 0:
                        save_progress(progress_file, processed_files, total_success, total_failed, failed_files)
                        print(f"ğŸ’¾ å·²ä¿å­˜é€²åº¦ ({i}/{len(tasks)})")
                        
                except Exception as e:
                    total_failed += 1
                    failed_files.append({
                        'file': str(pdf_path),
                        'error': str(e),
                        'timestamp': time.time()
                    })
                    print(f"âŒ ä»»å‹™åŸ·è¡Œå¤±æ•—: {pdf_path.name}")
                    print(f"   éŒ¯èª¤: {str(e)}")
                    
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·è½‰æ›")
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜é€²åº¦...")
        save_progress(progress_file, processed_files, total_success, total_failed, failed_files)
        print(f"ğŸ“‹ é€²åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡åŸ·è¡Œå°‡å¾æœªå®Œæˆçš„æª”æ¡ˆé–‹å§‹")
        return
    
    print(f"\n{'='*50}")
    print(f"ğŸ‰ ä¸¦è¡Œæ‰¹æ¬¡è½‰æ›å®Œæˆå ±å‘Š")
    print(f"ğŸ“„ è™•ç†æª”æ¡ˆ: {len(pdfs_to_process)} å€‹")
    print(f"âœ… ç¸½æˆåŠŸ: {total_success} å€‹æª”æ¡ˆ")
    print(f"âŒ ç¸½å¤±æ•—: {total_failed} å€‹æª”æ¡ˆ")
    print(f"ğŸš€ ä½¿ç”¨å·¥ä½œé€²ç¨‹: {max_workers} å€‹")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_base_dir}")
    print(f"ğŸ–¼ï¸  åœ–ç‰‡ç›®éŒ„: {image_output_dir}")
    
    # ä¿å­˜å¤±æ•—æª”æ¡ˆæ¸…å–®
    if failed_files:
        with open(failed_files_file, 'w', encoding='utf-8') as f:
            json.dump(failed_files, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ å¤±æ•—æª”æ¡ˆæ¸…å–®å·²ä¿å­˜è‡³: {failed_files_file}")
    
    if progress_file.exists():
        progress_file.unlink()
        print(f"ğŸ§¹ å·²æ¸…ç†é€²åº¦æª”æ¡ˆ")
    
    if output_base_dir.exists():
        print(f"\nğŸ“‚ è¼¸å‡ºç›®éŒ„çµæ§‹:")
        for output_file in output_base_dir.rglob("*.md"):
            rel_path = output_file.relative_to(output_base_dir)
            print(f"   ğŸ“„ {rel_path}")

def batch_convert_exclude_chinese_english(root_dir, output_base_dir, image_output_dir, model_cache_path, device, encoding, enable_math_processing, enable_multilingual_ocr, extract_table_format):
    """é †åºæ‰¹æ¬¡è½‰æ›ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
    excluded_subjects = {"Chinese", "English"}
    progress_file = output_base_dir / "conversion_progress.json"
    failed_files_file = output_base_dir / "failed_files.json"
    processed_files = set()
    failed_files = []
    
    if progress_file.exists():
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
                processed_files = set(progress_data.get('processed_files', []))
                failed_files = progress_data.get('failed_files', [])
                print(f"ğŸ“‹ ç™¼ç¾é€²åº¦æª”æ¡ˆï¼Œå·²è™•ç† {len(processed_files)} å€‹æª”æ¡ˆ")
                print(f"âŒ å¤±æ•—æª”æ¡ˆ: {len(failed_files)} å€‹")
                print(f"ğŸ”„ å°‡å¾æœªè™•ç†çš„æª”æ¡ˆé–‹å§‹ç¹¼çºŒè½‰æ›")
        except Exception as e:
            print(f"âš ï¸  è®€å–é€²åº¦æª”æ¡ˆå¤±æ•—: {e}")
    
    all_pdfs = list(Path(root_dir).rglob("*.pdf"))
    print(f"\nğŸ” æƒæç›®éŒ„: {root_dir}")
    print(f"ğŸ“„ æ‰¾åˆ° {len(all_pdfs)} å€‹PDFæª”æ¡ˆ")
    print(f"ğŸš« æ’é™¤ç§‘ç›®: {', '.join(excluded_subjects)}")
    pdfs_to_process = []
    for pdf_path in all_pdfs:
        path_parts = pdf_path.parts
        root_parts = Path(root_dir).parts
        if len(path_parts) > len(root_parts):
            subject_name = path_parts[-2]
            if subject_name not in excluded_subjects:
                if str(pdf_path) not in processed_files:
                    pdfs_to_process.append(pdf_path)
                else:
                    print(f"â­ï¸  è·³éå·²è™•ç†çš„æª”æ¡ˆ: {pdf_path.name}")
            else:
                print(f"â­ï¸  è·³é {subject_name} ç§‘ç›®æª”æ¡ˆ: {pdf_path.name}")
        else:
<<<<<<< HEAD
            print(f"âš ï¸  è·³éè·¯å¾‘çµæ§‹ä¸æ­£ç¢ºçš„æª”æ¡ˆ: {pdf_path}")
    print(f"âœ… å°‡è™•ç† {len(pdfs_to_process)} å€‹PDFæª”æ¡ˆ")
    if len(pdfs_to_process) == 0:
        print("ğŸ‰ æ‰€æœ‰æª”æ¡ˆéƒ½å·²è™•ç†å®Œæˆï¼")
        return
    extractor = create_pdf_page_extractor(
        device=device,
        model_dir_path=str(model_cache_path),
        extract_formula=True,
        extract_table_format=extract_table_format,
    )
    if not extractor:
        print("âŒ PDFè§£æå™¨åˆå§‹åŒ–å¤±æ•—")
        return
    total_success = 0
    total_failed = 0
    output_base_dir.mkdir(parents=True, exist_ok=True)
    try:
        for i, pdf_path in enumerate(pdfs_to_process, 1):
            print(f"\n{'='*20} è™•ç†æª”æ¡ˆ {i}/{len(pdfs_to_process)} {'='*20}")
            print(f"ğŸ“„ æª”æ¡ˆ: {pdf_path.name}")
            print(f"ğŸ“ è·¯å¾‘: {pdf_path}")
            rel_path = pdf_path.parent.relative_to(root_dir)
            output_dir = output_base_dir / rel_path
            image_dir = image_output_dir / rel_path
            print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {output_dir}")
            print(f"ğŸ–¼ï¸  åœ–ç‰‡ç›®éŒ„: {image_dir}")
            success, output_path = convert_pdf_to_markdown(
                pdf_path,
                output_dir,
                image_dir,
                extractor,
                encoding=encoding,
                enable_math_processing=enable_math_processing,
                enable_multilingual_ocr=enable_multilingual_ocr
            )
            processed_files.add(str(pdf_path))
            if success:
                total_success += 1
                print(f"âœ… æˆåŠŸè½‰æ›: {pdf_path.name}")
            else:
                total_failed += 1
                failed_files.append({
                    'file': str(pdf_path),
                    'error': 'è½‰æ›å¤±æ•—',
                    'timestamp': time.time()
                })
                print(f"âŒ è½‰æ›å¤±æ•—: {pdf_path.name}")
            if i % 10 == 0:
                save_progress(progress_file, processed_files, total_success, total_failed, failed_files)
                print(f"ğŸ’¾ å·²ä¿å­˜é€²åº¦ ({i}/{len(pdfs_to_process)})")
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·è½‰æ›")
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜é€²åº¦...")
        save_progress(progress_file, processed_files, total_success, total_failed, failed_files)
        print(f"ğŸ“‹ é€²åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡åŸ·è¡Œå°‡å¾æœªå®Œæˆçš„æª”æ¡ˆé–‹å§‹")
        return
    print(f"\n{'='*50}")
    print(f"ğŸ‰ æ‰¹æ¬¡è½‰æ›å®Œæˆå ±å‘Š")
    print(f"ğŸ“„ è™•ç†æª”æ¡ˆ: {len(pdfs_to_process)} å€‹")
    print(f"âœ… ç¸½æˆåŠŸ: {total_success} å€‹æª”æ¡ˆ")
    print(f"âŒ ç¸½å¤±æ•—: {total_failed} å€‹æª”æ¡ˆ")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_base_dir}")
    print(f"ğŸ–¼ï¸  åœ–ç‰‡ç›®éŒ„: {image_output_dir}")
    
    # ä¿å­˜å¤±æ•—æª”æ¡ˆæ¸…å–®
    if failed_files:
        with open(failed_files_file, 'w', encoding='utf-8') as f:
            json.dump(failed_files, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ å¤±æ•—æª”æ¡ˆæ¸…å–®å·²ä¿å­˜è‡³: {failed_files_file}")
    
    if progress_file.exists():
        progress_file.unlink()
        print(f"ğŸ§¹ å·²æ¸…ç†é€²åº¦æª”æ¡ˆ")
    if output_base_dir.exists():
        print(f"\nğŸ“‚ è¼¸å‡ºç›®éŒ„çµæ§‹:")
        for output_file in output_base_dir.rglob("*.md"):
            rel_path = output_file.relative_to(output_base_dir)
            print(f"   ğŸ“„ {rel_path}")

def save_progress(progress_file, processed_files, total_success, total_failed, failed_files=None):
    progress_data = {
        'processed_files': list(processed_files),
        'total_success': total_success,
        'total_failed': total_failed,
        'failed_files': failed_files or [],
        'timestamp': time.time()
    }
    try:
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜é€²åº¦å¤±æ•—: {e}")

def main():
    base_input_dir = Path("input_docs")
    output_base_dir = Path("output_docs/test_batch_all_exclude_chinese_english")
    image_output_dir = Path("images/test_batch_all_exclude_chinese_english")
=======
            fail_count += 1
    print(f"\nğŸ“Š æ‰¹æ¬¡è½‰æ›å®Œæˆï¼šæˆåŠŸ {success_count}ï¼Œå¤±æ•— {fail_count}")

def main():
    # === è¨­å®šè·¯å¾‘ ===
    base_input_dir = Path("input_docs")  # è™•ç† input_docs åº•ä¸‹æ‰€æœ‰æª”æ¡ˆ
    output_base_dir = Path("output_docs")
    image_output_dir = Path("images")
>>>>>>> c5fd878ef717c3e7aee6fd715ea2cfcec3472816
    model_cache_path = Path("model")
    device, gpu_count, gpu_memory = detect_gpu()
    encoding = "utf-8"
    enable_math_processing = True
    enable_multilingual_ocr = True
<<<<<<< HEAD
    extract_table_format = ExtractedTableFormat.DISABLE
    
    # å¤šå·¥è™•ç†è¨­å®š
    use_parallel = True  # è¨­ç‚ºFalseä½¿ç”¨é †åºè™•ç†
    max_workers = None   # Noneè¡¨ç¤ºè‡ªå‹•è¨ˆç®—ï¼Œæˆ–æŒ‡å®šæ•¸å­—å¦‚4
    
    print(f"ğŸš€ é–‹å§‹PDFè½‰æ›ä»»å‹™")
    print(f"   ğŸ“ è¼¸å…¥ç›®éŒ„: {base_input_dir}")
    print(f"   ğŸ“ è¼¸å‡ºç›®éŒ„: {output_base_dir}")
    print(f"   ğŸ–¼ï¸  åœ–ç‰‡ç›®éŒ„: {image_output_dir}")
    print(f"   ğŸ”§ è¨­å‚™: {device}")
    print(f"   ğŸ“ æ•¸å­¸å…¬å¼è™•ç†: {'å•Ÿç”¨' if enable_math_processing else 'åœç”¨'}")
    print(f"   ğŸŒ å¤šèªè¨€OCR: {'å•Ÿç”¨' if enable_multilingual_ocr else 'åœç”¨'}")
    print(f"   ğŸ“Š è¡¨æ ¼æå–: åœç”¨ (é¿å…æ¨¡çµ„ç¼ºå¤±éŒ¯èª¤)")
    print(f"   ğŸš€ ä¸¦è¡Œè™•ç†: {'å•Ÿç”¨' if use_parallel else 'åœç”¨'}")
    
    try:
        if use_parallel:
            batch_convert_parallel(
                base_input_dir,
                output_base_dir,
                image_output_dir,
                model_cache_path,
                device,
                encoding,
                enable_math_processing,
                enable_multilingual_ocr,
                extract_table_format,
                max_workers
            )
        else:
            batch_convert_exclude_chinese_english(
                base_input_dir,
                output_base_dir,
                image_output_dir,
                model_cache_path,
                device,
                encoding,
                enable_math_processing,
                enable_multilingual_ocr,
                extract_table_format
            )
    except Exception as e:
        print(f"âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        error_details = traceback.format_exc()
        logging.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—\néŒ¯èª¤è©³æƒ…: {error_details}")
        return 1
    print(f"ğŸ‰ ç¨‹å¼åŸ·è¡Œå®Œæˆ")
    return 0
=======
    extract_table_format = ExtractedTableFormat.MARKDOWN
    # === å–®ç·šç¨‹æ‰¹æ¬¡è™•ç†æ‰€æœ‰ PDF ===
    batch_convert_all_pdfs(
        base_input_dir,
        output_base_dir,
        image_output_dir,
        model_cache_path,
        device,
        encoding,
        enable_math_processing,
        enable_multilingual_ocr,
        extract_table_format
    )
>>>>>>> c5fd878ef717c3e7aee6fd715ea2cfcec3472816

if __name__ == "__main__":
    main()